{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5769be82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbe1055",
   "metadata": {},
   "source": [
    "# 1. 데이터 로드 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "273ff224",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './data/ChatbotData .csv'\n",
    "data = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85fdb157",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = data['Q'].tolist()\n",
    "answers = data['A'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c36d914",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^가-힣a-zA-Z?.!,]+\", \" \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75044324",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [preprocess_sentence(sentence) for sentence in questions]\n",
    "answers = [preprocess_sentence(sentence) for sentence in answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecfa6a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = [\"<start> \" + answer + \" <end>\" for answer in answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77797375",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "tokenizer.fit_on_texts(questions + answers)\n",
    "\n",
    "VOCAB_SIZE = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47890d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_seq = tokenizer.texts_to_sequences(questions)\n",
    "answers_seq = tokenizer.texts_to_sequences(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d292d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 40  \n",
    "questions_seq = tf.keras.preprocessing.sequence.pad_sequences(questions_seq, maxlen=MAX_LENGTH, padding='post')\n",
    "answers_seq = tf.keras.preprocessing.sequence.pad_sequences(answers_seq, maxlen=MAX_LENGTH, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19543ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_train, questions_val, answers_train, answers_val = train_test_split(questions_seq, answers_seq, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d296811",
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_inputs_train = answers_train[:, :-1]\n",
    "dec_inputs_val = answers_val[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b35f2a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_train_shifted = answers_train[:, 1:]\n",
    "answers_val_shifted = answers_val[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7de3dd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 20000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((questions_train, dec_inputs_train, answers_train_shifted))\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((questions_val, dec_inputs_val, answers_val_shifted))\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8702115a",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data['label'].values  \n",
    "\n",
    "questions_train, questions_val, answers_train, answers_val, labels_train, labels_val = train_test_split(\n",
    "    questions_seq, answers_seq, labels, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0404e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices(((questions_train, dec_inputs_train), labels_train))\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(((questions_val, dec_inputs_val), labels_val))\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63daf16e",
   "metadata": {},
   "source": [
    "# 2. 트랜스포머 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "270cdac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "\n",
    "    attention = tf.keras.layers.MultiHeadAttention(\n",
    "        num_heads=num_heads, key_dim=d_model, dropout=dropout)(inputs, inputs)\n",
    "    attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
    "    attention = tf.keras.layers.LayerNormalization(epsilon=1e-6)(inputs + attention)\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention + outputs)\n",
    "\n",
    "    return tf.keras.Model(inputs=inputs, outputs=outputs, name=name)\n",
    "\n",
    "def encoder(vocab_size, num_layers, units, d_model, num_heads, dropout, name=\"encoder\"):\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "    embeddings = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "    outputs = embeddings\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        outputs = encoder_layer(units, d_model, num_heads, dropout, name=f\"encoder_layer_{i}\")(outputs)\n",
    "\n",
    "    return tf.keras.Model(inputs=[inputs, padding_mask], outputs=outputs, name=name)\n",
    "\n",
    "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "    enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
    "    look_ahead_mask = tf.keras.Input(shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "    attention1 = tf.keras.layers.MultiHeadAttention(\n",
    "        num_heads=num_heads, key_dim=d_model, dropout=dropout)(inputs, inputs, attention_mask=look_ahead_mask)\n",
    "    attention1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention1 + inputs)\n",
    "\n",
    "    attention2 = tf.keras.layers.MultiHeadAttention(\n",
    "        num_heads=num_heads, key_dim=d_model, dropout=dropout)(attention1, enc_outputs, attention_mask=padding_mask)\n",
    "    attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
    "    attention2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention2 + attention1)\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention2)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(epsilon=1e-6)(outputs + attention2)\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "        outputs=outputs,\n",
    "        name=name)\n",
    "\n",
    "def decoder(vocab_size, num_layers, units, d_model, num_heads, dropout, name=\"decoder\"):\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "    enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
    "    look_ahead_mask = tf.keras.Input(shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "    embeddings = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "    outputs = embeddings\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        outputs = decoder_layer(units, d_model, num_heads, dropout, name=f\"decoder_layer_{i}\")(\n",
    "            inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "        outputs=outputs,\n",
    "        name=name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46283f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def create_look_ahead_mask(seq):\n",
    "    seq_len = tf.shape(seq)[1]\n",
    "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "    look_ahead_mask = look_ahead_mask[tf.newaxis, tf.newaxis, :, :]\n",
    "    return look_ahead_mask  \n",
    "\n",
    "\n",
    "def transformer(vocab_size, num_layers, units, d_model, num_heads, dropout, name=\"transformer\"):\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "    dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
    "\n",
    "    enc_padding_mask = tf.keras.layers.Lambda(create_padding_mask, output_shape=(1, 1, None), name='enc_padding_mask')(inputs)\n",
    "    look_ahead_mask = tf.keras.layers.Lambda(lambda x: create_look_ahead_mask(x), output_shape=(1, None, None), name='look_ahead_mask')(dec_inputs)\n",
    "    dec_padding_mask = tf.keras.layers.Lambda(create_padding_mask, output_shape=(1, 1, None), name='dec_padding_mask')(inputs)\n",
    "\n",
    "    enc_outputs = encoder(\n",
    "        vocab_size=vocab_size,\n",
    "        num_layers=num_layers,\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "    )(inputs=[inputs, enc_padding_mask])\n",
    "\n",
    "    dec_outputs = decoder(\n",
    "        vocab_size=vocab_size,\n",
    "        num_layers=num_layers,\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "    )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(units=3, activation='softmax', name=\"outputs\")(dec_outputs[:, -1, :])\n",
    "\n",
    "    return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3db1be3",
   "metadata": {},
   "source": [
    "# 3. 학습 설정 및 컴파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e87d5a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inputs (InputLayer)             [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dec_inputs (InputLayer)         [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Functional)            (None, None, 256)    9982208     inputs[0][0]                     \n",
      "                                                                 enc_padding_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "look_ahead_mask (Lambda)        (1, 1, None, None)   0           dec_inputs[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dec_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Functional)            (None, None, 256)    14190336    dec_inputs[0][0]                 \n",
      "                                                                 encoder[0][0]                    \n",
      "                                                                 look_ahead_mask[0][0]            \n",
      "                                                                 dec_padding_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem (Slici (None, 256)          0           decoder[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "outputs (Dense)                 (None, 3)            771         tf.__operators__.getitem[0][0]   \n",
      "==================================================================================================\n",
      "Total params: 24,173,315\n",
      "Trainable params: 24,173,315\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "NUM_LAYERS = 2\n",
    "D_MODEL = 256\n",
    "NUM_HEADS = 8\n",
    "UNITS = 512\n",
    "DROPOUT = 0.1\n",
    "\n",
    "model = transformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    units=UNITS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT)\n",
    "\n",
    "def loss_function(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1,))\n",
    "    return tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)(y_true, y_pred)\n",
    "\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps**-1.5)\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "learning_rate = CustomSchedule(D_MODEL)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "    return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7596fb28",
   "metadata": {},
   "source": [
    "# 4. 모델 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34c99ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/keras/backend.py:4906: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148/148 [==============================] - 22s 115ms/step - loss: 1.1783 - accuracy: 0.3916 - val_loss: 1.0559 - val_accuracy: 0.4474\n",
      "Epoch 2/10\n",
      "148/148 [==============================] - 17s 112ms/step - loss: 0.9677 - accuracy: 0.5202 - val_loss: 0.9321 - val_accuracy: 0.5581\n",
      "Epoch 3/10\n",
      "148/148 [==============================] - 17s 114ms/step - loss: 0.7826 - accuracy: 0.6547 - val_loss: 0.7582 - val_accuracy: 0.6786\n",
      "Epoch 4/10\n",
      "148/148 [==============================] - 17s 115ms/step - loss: 0.5823 - accuracy: 0.7660 - val_loss: 0.6986 - val_accuracy: 0.7290\n",
      "Epoch 5/10\n",
      "148/148 [==============================] - 17s 114ms/step - loss: 0.4011 - accuracy: 0.8494 - val_loss: 0.7767 - val_accuracy: 0.7285\n",
      "Epoch 6/10\n",
      "148/148 [==============================] - 17s 113ms/step - loss: 0.2692 - accuracy: 0.9055 - val_loss: 0.7659 - val_accuracy: 0.7404\n",
      "Epoch 7/10\n",
      "148/148 [==============================] - 17s 113ms/step - loss: 0.1678 - accuracy: 0.9440 - val_loss: 1.2004 - val_accuracy: 0.7230\n",
      "Epoch 8/10\n",
      "148/148 [==============================] - 17s 113ms/step - loss: 0.1177 - accuracy: 0.9605 - val_loss: 1.1951 - val_accuracy: 0.7053\n",
      "Epoch 9/10\n",
      "148/148 [==============================] - 17s 114ms/step - loss: 0.0711 - accuracy: 0.9780 - val_loss: 1.4907 - val_accuracy: 0.7091\n",
      "Epoch 10/10\n",
      "148/148 [==============================] - 17s 114ms/step - loss: 0.0656 - accuracy: 0.9801 - val_loss: 1.5800 - val_accuracy: 0.7383\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7d01f960ee50>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "model.fit(train_dataset, epochs=EPOCHS, validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1374d7",
   "metadata": {},
   "source": [
    "# 5. 챗봇 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "96e2e62c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: 이별한 지 열흘 되었어요\n",
      "챗봇: 이별 관련 대화입니다.\n",
      "\n",
      "질문: 1지망 학교 떨어졌어\n",
      "챗봇: 일상적인 대화입니다.\n",
      "\n",
      "질문: 사랑에 빠진 것 같아요.\n",
      "챗봇: 사랑에 관한 대화네요.\n",
      "\n",
      "질문: 최근에 힘든 이별을 겪었어요.\n",
      "챗봇: 이별 관련 대화입니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response_dict = {\n",
    "    0: \"일상적인 대화입니다.\",\n",
    "    1: \"이별 관련 대화입니다.\",\n",
    "    2: \"사랑에 관한 대화네요.\"\n",
    "}\n",
    "\n",
    "def predict_label(sentence):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    sentence_seq = tokenizer.texts_to_sequences([sentence])\n",
    "    sentence_seq = tf.keras.preprocessing.sequence.pad_sequences(sentence_seq, maxlen=MAX_LENGTH, padding='post')\n",
    "\n",
    "    predictions = model([sentence_seq, sentence_seq], training=False)\n",
    "    predicted_label = tf.argmax(predictions, axis=-1).numpy()[0]\n",
    "\n",
    "    return predicted_label\n",
    "\n",
    "test_sentences = [\n",
    "    \"이별한 지 열흘 되었어요\",\n",
    "    \"1지망 학교 떨어졌어\",\n",
    "    \"사랑에 빠진 것 같아요.\",\n",
    "    \"최근에 힘든 이별을 겪었어요.\"\n",
    "]\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    predicted_label = predict_label(sentence)\n",
    "    response = response_dict[predicted_label]\n",
    "    print(f\"질문: {sentence}\")\n",
    "    print(f\"챗봇: {response}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fc7ad5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
